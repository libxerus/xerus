// Xerus - A General Purpose Tensor Library
// Copyright (C) 2014-2019 Benjamin Huber, Sebastian Wolf, Michael GÃ¶tte.
// 
// Xerus is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as published
// by the Free Software Foundation, either version 3 of the License,
// or (at your option) any later version.
// 
// Xerus is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
// GNU Affero General Public License for more details.
// 
// You should have received a copy of the GNU Affero General Public License
// along with Xerus. If not, see <http://www.gnu.org/licenses/>.
//
// For further information on Xerus visit https://libXerus.org 
// or contact us at contact@libXerus.org.

/**
* @file
* @brief Header file for the HTNetwork class (and thus HTTensor and HTOperator).
*/

#pragma once

 
#include "misc/containerSupport.h"
#include "misc/check.h"

#include "index.h"
#include "indexedTensor_tensor_factorisations.h"
#include "tensor.h"
#include "tensorNetwork.h"
#include "indexedTensor.h"
#include "indexedTensorMoveable.h"
#include "indexedTensorList.h"

//#include <xerus/misc/internal.h>




namespace xerus {
	/**
	* @brief Specialized TensorNetwork class used to represent balanced HTTensor and HToperators.
	* @details HTTensors correspond to isOperator=FALSE and HTOperators correspond to isOperator=FALSE.
	*/
	template<bool isOperator>
	class HTNetwork final : public TensorNetwork {
	public:
		///@brief The number of external links in each physical node, i.e. one for HTTensors and two for HTOperators.
		static constexpr const size_t N = isOperator?2:1;
		
		/// @brief Flag indicating whether the HTNetwork is canonicalized.
		bool canonicalized;
		
		/**
		* @brief The position of the core.
		* @details If canonicalized is TRUE, corePosition gives the position of the core tensor. All other components
		* are orthoginal with respect to the edge closest to the core.
		*/
		size_t corePosition;		
		

		/*- - - - - - - - - - - - - - - - - - - - - - - - - - Constructors - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -*/
		/** 
		* @brief Constructs an order zero HTNetwork.
		* @details This is an empty TensorNetwork. Internally the network contains one order zero node with entry zero.
		*/
		explicit HTNetwork();
		
		
		///@brief HTNetworks are default copy constructable.
		HTNetwork(const HTNetwork & _cpy) = default;
		
		
		///@brief HTNetworks are default move constructable.
		HTNetwork(      HTNetwork&& _mov) noexcept = default;
		
		
		/** 
		* @brief Constructs an zero initialized HTNetwork with the given order and ranks all equal to one.
		* @details Naturally for HTOperators the order must be even.
		* @param _order number of physical indices
		*/
		explicit HTNetwork(const size_t _order);
		
		
		/** 
		* @brief Constructs an zero initialized HTNetwork with the given dimensions and ranks all equal to one.
		* @details Naturally for HTOperators the order must be even.
		* @params _dimensions Tuple of the dimensions of the physical indices
		*/
		explicit HTNetwork(Tensor::DimensionTuple _dimensions);
		
		
		/** 
		* @brief Constructs a HTNetwork from the given Tensor.
		* @details  The higher order SVD algorithm is used to decompose the given Tensor into the HT format.
		* @param _tensor The Tensor to decompose.
		* @param _eps the accuracy to be used in the decomposition.
		* @param _maxRank the maximal allowed rank (applies to all positions).
		*/
		explicit HTNetwork(const Tensor& _tensor, const double _eps=EPSILON, const size_t _maxRank=std::numeric_limits<size_t>::max());
		
		
		/** 
		* @brief Constructs a HTNetwork from the given Tensor.
		* @details  The higher order SVD algorithm is used to decompose the given Tensor into the HT format.
		* @param _tensor The Tensor to decompose.
		* @param _eps the accuracy to be used in the decomposition.
		* @param _maxRanks maximal ranks to be used
		*/
		explicit HTNetwork(const Tensor& _tensor, const double _eps, const RankTuple& _maxRanks);
		
		
		
		/** 
		 * @brief Random constructs a HTNetwork with the given dimensions and ranks.
		 * @details The entries of the component tensors are sampled independently using the provided random generator and distribution.
		 * @param _dimensions the dimensions of the to be created HTNetwork.
		 * @param _ranks the ranks of the to be created HTNetwork.
		 * @param _rnd the random engine to be passed to the constructor of the component tensors.
		 * @param _dist the random distribution to be passed to the constructor of the component tensors.
		 */
		template<class distribution=std::normal_distribution<value_t>, class generator=std::mt19937_64>
		static HTNetwork XERUS_warn_unused random(std::vector<size_t> _dimensions, const std::vector<size_t> &_ranks, distribution& _dist=xerus::misc::defaultNormalDistribution, generator& _rnd=xerus::misc::randomEngine) {

			const size_t numOfLeaves = _dimensions.size()/N;
			const size_t numIntComp = numOfLeaves - 1;

			const size_t numComponents = numIntComp + numOfLeaves;

			XERUS_REQUIRE(_dimensions.size()%N==0, "Illegal number of dimensions/Leaves for HTOperator.");
			XERUS_REQUIRE(_ranks.size()+1 == numComponents,"Non-matching amount of ranks given to HTNetwork::random.");
			XERUS_REQUIRE(numIntComp >= 0,"No internal Components! ");
			XERUS_REQUIRE(!misc::contains(_dimensions, size_t(0)), "Trying to construct a HTTensor with dimension 0 is not possible.");
			XERUS_REQUIRE(!misc::contains(_ranks, size_t(0)), "Trying to construct random HTTensor with rank 0 is illegal.");


			HTNetwork result(_dimensions.size());

			//const std::vector<size_t> targetRank = reduce_to_maximal_ranks(_ranks, _dimensions);
			const std::vector<size_t> targetRank = std::move(_ranks);

			for(size_t i = 0; i < numComponents; ++i) {
				//Create inner components
				if (i < numIntComp){
					const size_t child1Rank = 2 * i >= numComponents - 1 ? 1 : targetRank[2 * i];
					const size_t child2Rank = 2 * i + 1 >= numComponents - 1 ? 1 : targetRank[2 * i + 1];
					const size_t parentRank = i == 0 ? 1 : targetRank[i - 1];
					const auto rndComp = Tensor::random({parentRank, child1Rank, child2Rank}, _dist, _rnd);
					result.set_component(i, rndComp);
				}
				else {
					const size_t parentRank = targetRank[i - 1];
					if(isOperator) {
						const auto rndComp = Tensor::random({parentRank, _dimensions[i - numIntComp], _dimensions[numOfLeaves + i - numIntComp]}, _dist, _rnd);
						result.set_component(i, rndComp);
					} else {
						const auto rndComp = Tensor::random({parentRank, _dimensions[i - numIntComp]}, _dist, _rnd);
						result.set_component(i, rndComp);
					}
				}
			}
			result.move_core(0);
			return result;
		}
		
		
		/** 
		 * @brief Random constructs a HTNetwork with the given dimensions and ranks limited by the given rank.
		 * @details The entries of the component tensors are sampled independently using the provided random generator and distribution.
		 * @param _dimensions the dimensions of the to be created HTNetwork.
		 * @param _rank the maximal allowed rank. 
		 * @param _rnd the random engine to be passed to the constructor of the component tensors.
		 * @param _dist the random distribution to be passed to the constructor of the component tensors.
		 */
		template<class distribution=std::normal_distribution<value_t>, class generator=std::mt19937_64>
		static HTNetwork XERUS_warn_unused random(const std::vector<size_t>& _dimensions, const size_t _rank, distribution& _dist=xerus::misc::defaultNormalDistribution, generator& _rnd=xerus::misc::randomEngine) {
			return HTNetwork::random(_dimensions, std::vector<size_t>(2 * _dimensions.size()/N - 2, _rank), _dist, _rnd);
		}
		
		
		/** 
		 * @brief: Returns a  (rank one) HTTensor with all entries equal to one.
		 * @param _dimensions the dimensions of the new tensor.
		 */
		static HTNetwork XERUS_warn_unused ones(const std::vector<size_t>& _dimensions);
		
		
		/** 
		 * @brief: Construct a HTOperator with the given dimensions representing the identity.
		 * @details Only applicable for HTOperators, i.e. not for HTTensosr
		 * @param _dimensions the dimensions of the new HTOperator.
		 */
		template<bool B = isOperator, typename std::enable_if<B, int>::type = 0>
		static HTNetwork XERUS_warn_unused identity(const std::vector<size_t>& _dimensions);

		
		/** 
		 * @brief: Returns a HTNetwork representation of the kronecker delta.
		 * @details That is each entry is one if all indices are equal and zero otherwise. Note iff d=2 this coincides with identity.
		 * @param _dimensions the dimensions of the new tensor.
		 */
//		static HTNetwork XERUS_warn_unused kronecker(const std::vector<size_t>& _dimensions);
		
		
		/** 
		 * @brief: Returns a HTNetwork with a single entry equals one and all other zero.
		 * @param _dimensions the dimensions of the new tensor.
		 * @param _position The position of the one
		 */
		static HTNetwork XERUS_warn_unused dirac(std::vector<size_t> _dimensions, const std::vector<size_t>& _position);
		
		
		/** 
		 * @brief: Returns a HTNetwork with a single entry equals oen and all other zero.
		 * @param _dimensions the dimensions of the new tensor.
		 * @param _position The position of the one
		 */
		static HTNetwork XERUS_warn_unused dirac(std::vector<size_t> _dimensions, const size_t _position);
		
		/*- - - - - - - - - - - - - - - - - - - - - - - - - - Standard Operators - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -*/
		///@brief HTNetworks are default assignable.
		HTNetwork& operator=(const HTNetwork&  _other) = default;
		
		
		///@brief HTNetworks are default move-assignable.
		HTNetwork& operator=(HTNetwork&& _other) = default;
		
		
		/*- - - - - - - - - - - - - - - - - - - - - - - - - - Internal helper functions - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -*/
	protected:
		/**
		 * @brief Return the number of ranks, i.e. 0 for order zero and number of components -1 otherwise.
		 * @return number of ranks
		 */
		size_t num_ranks() const;
		
		/**
		 * @brief returns the path from one node to another in the binary tree
		 * @details this function is used to shift the core tensor along this path when the core tensor is moved, e.g. in a level 3 hierachical where 0 is the root
		 * and 3,4,5,6 are the leaves the path from 1 to 6 would be 1 -> 0 -> 2 -> 6
		 * @param _start node
		 * @param _end node
		 * @return path from start to end
		 */
		std::vector<size_t> get_path(size_t _start, size_t _end) const;

		/**
		 * @brief function to recursively find the path from the root to a destination
		 * @param _root starting point for the downward search is 0 for the first call
		 * @param _dest destination node
		 * @param _path path from root to dest
		 * @return returns true if dest was found
		 */
		bool get_path_from_root(size_t _root, size_t _dest, std::vector<size_t>& _path ) const;

		/**
		 * @brief function which returns the parent of a given component
		 * @details implements the root to leaves numbering of the HTNetwork
		 * @param _comp index of the component
		 * @return returns parent of a component
		 */
		size_t get_parent_component(size_t _comp) const;

		/**
		 * @brief function which returns the left child of a given component
		 * @details implements the root to leaves numbering of the HTNetwork
		 * @param _comp index of the component
		 * @return returns left child of a component
		 */
		size_t get_left_child_component(size_t _comp) const;

		/**
		 * @brief function which returns the right child of a given component
		 * @details implements the root to leaves numbering of the HTNetwork
		 * @param _comp index of the component
		 * @return returns right child of a component
		 */
		size_t get_right_child_component(size_t _comp) const;

		/**
		 * @brief function which returns true if the component is the left child of its parent component or false if it is the right child
		 * @param _comp index of the component
		 * @return bool if it is left child
		 */
		bool is_left_child(size_t _comp) const;

		/**
		 * @brief function which returns the number of components in an HTTensor
		 * @return numbe rof components
		 */
		size_t get_number_of_components() const;

		/*- - - - - - - - - - - - - - - - - - - - - - - - - - Miscellaneous - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -*/
	public:

		/*
		 * @brief Reduces the given ranks to the maximal possible.
		 * @details If a given rank is already smaller or equal it is left unchanged.
		 * @param _ranks the inital ranks to be reduced.
		 * @param _dimensions the dimensions used to calculate the maximal ranks.
		 * @return the reduced ranks.
		 */
		//static std::vector<size_t> reduce_to_maximal_ranks(std::vector<size_t> _ranks, const std::vector<size_t>& _dimensions);
		


		/**
		 * @brief Converts all components to use dense representations.
		 * @note This might be required because not all functionality of HTNetworks is available with sparse component tensors.
		 */
		void use_dense_representations();
		
		/** 
		* @brief Complete access to a specific component of the HT decomposition.
		* @note This function will not update rank and external dimension informations if it is used to set a component.
		* @details This function gives complete access to the components, only intended for internal use.
		* @param _idx index of the component to access.
		* @returns a reference to the requested component.
		*/
		Tensor& component(const size_t _idx);
		
		
		/** 
		* @brief Read access to a specific component of the HT decomposition.
		* @details This function should be used to access the components, instead of direct access via
		* nodes[...]
		* @param _idx index of the component to access.
		* @returns a const reference to the requested component.
		*/
		const Tensor& get_component(const size_t _idx) const;
		
		
		/** 
		* @brief Sets a specific component of the HT decomposition.
		* @details This function also takes care of adjusting the corresponding link dimensions and external dimensions
		* if needed. However this might still leave the HTNetwork in an invalid if the rank is changed. In this case it
		* is the callers responsibility to also update the other component tensors consistently to account for that rank
		* change.
		* @param _idx index of the component to set.
		* @param _T Tensor to use as the new component tensor.
		*/
		void set_component(const size_t _idx, Tensor _T);
		
		
		/** 
		* @brief Reduce all ranks up to a given accuracy and maximal number.
		* @param _maxRanks maximal allowed ranks. All current ranks that are larger than the given ones are reduced by truncation.
		* @param _eps the accuracy to use for truncation in the individual SVDs.
		*/
		void round(const std::vector<size_t>& _maxRanks, const double _eps = EPSILON);
		
		
		/** 
		* @brief Reduce all ranks to the given number.
		* @param _maxRank maximal allowed rank. All current ranks that are larger than this are reduced by truncation.
		*/
		void round(const size_t _maxRank);
		
		
		/** 
		* @brief Reduce all ranks to the given number.
		* @param _maxRank maximal allowed rank. All current ranks that are larger than this are reduced by truncation.
		*/
		void round(const int _maxRank);
		
		
		/** 
		* @brief Reduce all ranks up to a given accuracy.
		* @param _eps the accuracy to use for truncation in the individual SVDs.
		*/
		void round(const value_t _eps);
		
		
		/** 
		* @brief Applies the soft threshholding operation to all ranks.
		* @param _tau the soft threshholding parameter to be applied. I.e. all singular values are reduced to max(0, Lambda_ui - _tau).
		*/
		void soft_threshold(const double _tau, const bool _preventZero = false);
		
		
		/** 
		* @brief Applies soft threshholding operations to all ranks.
		* @param _taus the soft threshholding parameters to be applied. I.e. all singular values of the j-th matrification are reduced to max(0, Lambda_ui - _tau[j]).
		*/
		void soft_threshold(const std::vector<double>& _taus, const bool _preventZero = false);
		
		
		/** 
		* @brief Gets the ranks of the HTNetwork.
		* @return A vector containing the current ranks.
		*/
		std::vector<size_t> ranks() const;
		
		
		/** 
		* @brief Gets the rank of a specific edge of the HTNetwork.
		* @param _i Position of the edge in question.
		* @return The current rank of edge _i.
		*/
		size_t rank(const size_t _i) const;
		
		
		/** 
		* @brief Move the core to a new position.
		* @details The core is moved to @a _position and the nodes between the old and the new position are orthogonalized
		* accordingly. If the HTNetwork is not yet canonicalized it will be with @a _position as new corePosition.
		* @param _position the new core position.
		* @param _keepRank by default a rank revealing QR decomposition is used to move the core and the ranks are reduced
		* accordingly. If @a _keepRank is set the rank is not reduced, this is need e.g. in the ALS.
		*/
		void move_core(const size_t _position, const bool _keepRank=false);
		
		
		/**
		* @brief stores @a _pos as the current core position without verifying of ensuring that this is the case
		* @details this is particularly useful after constructing an own HT tensor with set_component calls
		* as these will assume that all orthogonalities are destroyed
		*/
		void assume_core_position(const size_t _pos);
		
		
		/** 
		* @brief Move the core to the root.
		* @details Basically calls move_core() with _position = 0
		*/
		void canonicalize_root();

		/** 
		* @brief Transpose the HTOperator
		* @details Swaps all external indices to create the transposed operator.
		*/
		template<bool B = isOperator, typename std::enable_if<B, int>::type = 0>
		void transpose() {
			const std::vector<size_t> shuffle({0,2,1});
			size_t numComp = get_number_of_components();
			//only leaves
			for (size_t n = numComp - 1; n >= numComp - order()/N; --n) {
				xerus::reshuffle(component(n), component(n), shuffle);
			}
		}
		
		
		virtual TensorNetwork* get_copy() const override;
		
		
		
		
		virtual value_t frob_norm() const override;
		
		
		/** 
		 * @brief Tests whether the network resembles that of a HTTensor and checks consistency with the underlying tensor objects.
		 * @details Note that this will NOT check for orthogonality of canonicalized HTNetworks.
		 */
		virtual void require_correct_format() const override;
		
		

		/*- - - - - - - - - - - - - - - - - - - - - - - - - -  Basic arithmetics - - - - - - - - - - - - - - - - - - - - - - - - - - */
		/** 
		* @brief Adds a given HTNetwork to this one.
		* @details To be well-defined it is required that the dimensions of this and @a _other coincide. 
		* The rank of the result are in general the entrywise sum of the ranks of this and @a _other.
		* @param _other the HTNetwork to add.
		* @return reference to this HTNetwork.
		*/
		HTNetwork& operator+=(const HTNetwork& _other);
		
		
		/** 
		* @brief Subtracts the @a _other HTNetwork entrywise from this one.
		* @details To be well-defined it is required that the dimensions of this and @a _other coincide. 
		* The rank of the result are in general the entrywise sum of the ranks of this and @a _other.
		* @param _other the Tensor to be subtracted to this one.
		* @return a reference to this HTNetwork.
		*/
		HTNetwork& operator-=(const HTNetwork& _other);
		
		/** 
		* @brief Calculates the entrywise multiplication of this TensorNetwork with a constant @a _factor.
		* @details Internally this only results in a change in the global factor.
		* @param _factor the factor.
		*/
		virtual void operator*=(const value_t _factor) override;
		
		/** 
		* @brief Calculates the entrywise divison of this TensorNetwork by a constant @a _divisor.
		* @details Internally this only results in a change in the global factor.
		* @param _divisor the divisor.
		*/
		virtual void operator/=(const value_t _divisor) override;
		
		/*- - - - - - - - - - - - - - - - - - - - - - - - - - Operator specializations - - - - - - - - - - - - - - - - - - - - - - - - - - */
		static bool specialized_contraction_f(std::unique_ptr<internal::IndexedTensorMoveable<TensorNetwork>>& _out, internal::IndexedTensorReadOnly<TensorNetwork>&& _me, internal::IndexedTensorReadOnly<TensorNetwork>&& _other);
		
		static bool specialized_sum_f(std::unique_ptr<internal::IndexedTensorMoveable<TensorNetwork>>& _out, internal::IndexedTensorReadOnly<TensorNetwork>&& _me, internal::IndexedTensorReadOnly<TensorNetwork>&& _other);
		
		virtual bool specialized_contraction(std::unique_ptr<internal::IndexedTensorMoveable<TensorNetwork>>& _out, internal::IndexedTensorReadOnly<TensorNetwork>&& _me, internal::IndexedTensorReadOnly<TensorNetwork>&& _other) const override {
			return specialized_contraction_f(_out, std::move(_me), std::move(_other));
		}

		virtual bool specialized_sum(std::unique_ptr<internal::IndexedTensorMoveable<TensorNetwork>>& _out, internal::IndexedTensorReadOnly<TensorNetwork>&& _me, internal::IndexedTensorReadOnly<TensorNetwork>&& _other) const override {
			return specialized_sum_f(_out, std::move(_me), std::move(_other));
		}

		virtual void specialized_evaluation(internal::IndexedTensorWritable<TensorNetwork>&& _me, internal::IndexedTensorReadOnly<TensorNetwork>&& _other) override;
		
	};

	using HTTensor = HTNetwork<false>;
	using HTOperator = HTNetwork<true>;


	/**
	* @brief Calculates the entrywise sum of the given HTNetworks @a _lhs and @a _rhs.
	* @details To be well-defined it is required that the dimensions of @a _lhs and @a _rhs coincide.
	* The rank of the result are in general the entrywise sum of the ranks of @a _lhs and @a _rhs.
	* @param _lhs the first summand.
	* @param _rhs the second summand.
	* @return the sum.
	*/
	template<bool isOperator>
	HTNetwork<isOperator> operator+(HTNetwork<isOperator> _lhs, const HTNetwork<isOperator>& _rhs);


	/**
	* @brief Calculates the entrywise difference of the given HTNetworks @a _lhs and @a _rhs.
	* @details To be well-defined it is required that the dimensions of @a _lhs and @a _rhs coincide.
	* The rank of the result are in general the entrywise sum of the ranks of @a _lhs and @a _rhs.
	* @param _lhs the minuend.
	* @param _rhs the subtrahend.
	* @return the difference.
	*/
	template<bool isOperator>
	HTNetwork<isOperator> operator-(HTNetwork<isOperator> _lhs, const HTNetwork<isOperator>& _rhs);


	/**
	* @brief Calculates the entrywise multiplication of the given HTNetwork @a _network with a constant @a _factor.
	* @details Internally this only results in a change in the global factor.
	* @param _network the HTNetwork,
	* @param _factor the factor,
	* @return the resulting scaled HTNetwork.
	*/
	template<bool isOperator>
	HTNetwork<isOperator> operator*(HTNetwork<isOperator> _network, const value_t _factor);


	/**
	* @brief Calculates the entrywise multiplication of the given HTNetwork @a _network with a constant @a _factor.
	* @details Internally this only results in a change in the global factor.
	* @param _factor the factor,
	* @param _network the HTNetwork,
	* @return the resulting scaled HTNetwork.
	*/
	template<bool isOperator>
	HTNetwork<isOperator> operator*(const value_t _factor, HTNetwork<isOperator> _network);


	/**
	* @brief Calculates the entrywise divison of this HTNetwork by a constant @a _divisor.
	* @details Internally this only results in a change in the global factor.
	* @param _network the HTNetwork
	* @param _divisor the divisor,
	* @return the resulting scaled HTNetwork.
	*/
	template<bool isOperator>
	HTNetwork<isOperator> operator/(HTNetwork<isOperator> _network, const value_t _divisor);



	namespace misc {

		/**
		* @brief Pipes all information necessary to restore the current TensorNetwork into @a _stream.
		* @note that this excludes header information
		*/
		template<bool isOperator>
		void stream_writer(std::ostream& _stream, const HTNetwork<isOperator> &_obj, misc::FileFormat _format);

		/**
		* @brief Restores the HTNetwork from a stream of data.
		*/
		template<bool isOperator>
		void stream_reader(std::istream& _stream, HTNetwork<isOperator> &_obj, const misc::FileFormat _format);
	}
}
